{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0YFX8xq29GU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48921216-0f8b-4f67-d6df-212ddcd2c90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install flashtext -q\n",
        "!pip install git+https://github.com/boudinfl/pke.git -q\n",
        "!pip install transformers -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install textwrap3 -q\n",
        "!pip install nltk -q\n",
        "!pip install strsim -q\n",
        "!pip install ipython-autotime -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install scipy -q\n",
        "!pip install networkx -q\n",
        "!pip install gradio -q\n",
        "# quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhiSpQoW3Cfy",
        "outputId": "4e3db02a-dcc5-4aea-e808-08345cf734e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "summary_model = summary_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sNOl5Cw3Cig",
        "outputId": "4bea2767-bb84-4240-99bf-285055bb4c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "Iy8mTz7C3rOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "def postprocesstext (content):\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "\n",
        "def summarizer(text,model,tokenizer):\n",
        "  text = text.strip().replace(\"\\n\",\" \")\n",
        "  text = \"summarize: \"+text\n",
        "  # print (text)\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=3,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  min_length = 75,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary= summary.strip()\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSnuhO753CqU",
        "outputId": "853798e2-a48a-42f9-aac5-2734b44aa1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key word"
      ],
      "metadata": {
        "id": "ZBjrfhV5354u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pke\n",
        "import traceback\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    out=[]\n",
        "    try:\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "        extractor.load_document(input=content,language='en')\n",
        "        #    not contain punctuation marks or stopwords as candidates.\n",
        "        pos = {'PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV', 'NUM'}\n",
        "        #pos = {'PROPN','NOUN'}\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "        extractor.candidate_selection(pos=pos)\n",
        "        # 4. build the Multipartite graph and rank candidates using random walk,\n",
        "        #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
        "        #    threshold/method parameters.\n",
        "        extractor.candidate_weighting(alpha=1.1,\n",
        "                                      threshold=0.75,\n",
        "                                      method='average')\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "\n",
        "\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "    except:\n",
        "        out = []\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from flashtext import KeywordProcessor\n",
        "\n",
        "def get_keywords(originaltext,summarytext):\n",
        "  keywords = get_nouns_multipartite(originaltext)\n",
        "  print (\"keywords unsummarized: \",keywords)\n",
        "  keyword_processor = KeywordProcessor()\n",
        "  for keyword in keywords:\n",
        "    keyword_processor.add_keyword(keyword)\n",
        "\n",
        "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "  keywords_found = list(set(keywords_found))\n",
        "  print (\"keywords_found in summarized: \",keywords_found)\n",
        "\n",
        "  important_keywords =[]\n",
        "  for keyword in keywords:\n",
        "    if keyword in keywords_found:\n",
        "      important_keywords.append(keyword)\n",
        "\n",
        "  return important_keywords[:4]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2CdPsaK3CyI",
        "outputId": "32e0fa7a-d772-46ba-8c56-1f78c526a0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5 model and Tokenizer import from Drive"
      ],
      "metadata": {
        "id": "SM9QBzGz4WC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/t5/model')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('/content/drive/MyDrive/t5/tokenizer')\n",
        "question_model = question_model.to(device)"
      ],
      "metadata": {
        "id": "SPmw3ARU3C5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation"
      ],
      "metadata": {
        "id": "KzGs9MjU61lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question(context,answer,model,tokenizer):\n",
        "  text = \"context: {} answer: {}\".format(context,answer)\n",
        "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n"
      ],
      "metadata": {
        "id": "9kfYZO_b3DBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distructor Generation"
      ],
      "metadata": {
        "id": "2-s1VTXaBJ0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_distractors_wordnet(word):\n",
        "    distractors=[]\n",
        "    try:\n",
        "      syn = wn.synsets(word,'n')[0]\n",
        "\n",
        "      word= word.lower()\n",
        "      orig_word = word\n",
        "      if len(word.split())>0:\n",
        "          word = word.replace(\" \",\"_\")\n",
        "\n",
        "      msq = syn.hyponyms()\n",
        "      hypernym = syn.hypernyms()\n",
        "      if len(msq) !=0:\n",
        "          for item in msq:\n",
        "            name = item.lemmas()[0].name()\n",
        "            if name == orig_word:\n",
        "                continue\n",
        "            name = name.replace(\"_\",\" \")\n",
        "            name = \" \".join(w.capitalize() for w in name.split())\n",
        "            if name is not None and name not in distractors:\n",
        "                distractors.append(name)\n",
        "      n=5-len(hypernym)\n",
        "\n",
        "      distractors=distractors[:n-1]\n",
        "      if len(hypernym)!=0:\n",
        "          distractors.append('other options------->')\n",
        "\n",
        "\n",
        "\n",
        "      if len(hypernym) == 0:\n",
        "          return distractors\n",
        "      for item in hypernym[0].hyponyms():\n",
        "          name = item.lemmas()[0].name()\n",
        "          #print (\"name \",name, \" word\",orig_word)\n",
        "          if name == orig_word:\n",
        "              continue\n",
        "          name = name.replace(\"_\",\" \")\n",
        "          name = \" \".join(w.capitalize() for w in name.split())\n",
        "          if name is not None and name not in distractors:\n",
        "              distractors.append(name)\n",
        "    except:\n",
        "      print (\"Wordnet distractors not found\")\n",
        "    return distractors\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kuVd-Js3DXY",
        "outputId": "a32f4b55-716a-4c86-d4b1-6a5b5b3a51ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "YHdXnTPiDCTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question(context,radiobutton):\n",
        "  summary_text = summarizer(context,summary_model,summary_tokenizer)\n",
        "  nounphrases =  get_keywords(context,summary_text)\n",
        "  output=\"\"\n",
        "  for answer in nounphrases:\n",
        "    ques = get_question(summary_text,answer,question_model,question_tokenizer)\n",
        "\n",
        "    distractors = get_distractors_wordnet(answer)\n",
        "\n",
        "    # output= output + ques + \"\\n\" + \"Ans: \"+answer.capitalize() + \"\\n\\n\"\n",
        "    output = output + \"<b style='color:blue;'>\" + ques + \"</b>\"\n",
        "    output = output + \"<br>\" + \"<b style='color:green;'>\" + 'correct options------->' + \"<br>\"\n",
        "    output = output + \"<b style='color:green;'>\" +answer.capitalize()+  \"</b>\"+\"<br>\"\n",
        "\n",
        "    if len(distractors)>0:\n",
        "      for distractor in distractors[:8]:\n",
        "        output = output + \"<b style='color:green;'>\" + distractor+  \"</b>\"+\"<br>\"\n",
        "    output = output + \"<br>\"\n",
        "\n",
        "  return output\n",
        "  # print(output)\n"
      ],
      "metadata": {
        "id": "ttRNkHMjDDus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.Interface(fn=generate_question, inputs=gr.Textbox(lines=10, placeholder=\"Enter paragraph/content here...\"), outputs=gr.HTML(  label=\"Question and Answers\"))\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "1RNJx5jTDRdk",
        "outputId": "fdbb5b84-d0eb-4c87-dd98-2a31ed7a8989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:819: UserWarning: Expected 2 arguments for function <function generate_question at 0x7ad14775b9a0>, received 1.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:823: UserWarning: Expected at least 2 arguments for function <function generate_question at 0x7ad14775b9a0>, received 1.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://19960602649bdc377e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://19960602649bdc377e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OD7CJLqfF8fE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}